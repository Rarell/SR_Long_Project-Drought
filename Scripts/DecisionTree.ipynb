{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a decision tree\n",
    "def DecisionTreeTrain(x, y, attributes, max_depth = 5, tree_depth = 0, multiplier = 1,\n",
    "                      tree = None, leaf = 0, A = None, a = None, A_best = None, n = None):\n",
    "    '''\n",
    "    Input:\n",
    "    x - Training examples. x is a m x n matrix, with n examples and m  features\n",
    "    y - Labels, or target values. Currently only binary 0 or 1 (or -1 and 1) allowed. Must be a vector of size m\n",
    "    attributes: List of numerical values to compare the training examples to. Currently, only a v x m array of \n",
    "                numerical values are accepted.\n",
    "    max_depth: Default = 5. Maxmium depth of the tree\n",
    "    multiplier: A constant to help better classify extreme cases. For the most common scenario, the most common is\n",
    "                (multipler * positive cases) compared to (negative cases)\n",
    "    \n",
    "    Output:\n",
    "    tree - Decision tree. It is formatted as an p x 5 array.\n",
    "           Column 1 is the estimated value for the label given that branch and for x < a\n",
    "           Column 2 is the feature (column) by which the data is split\n",
    "           Column 3 is the value (a) by which the data is split. \n",
    "                    For the estimated label, that is the value given the feature (a - 1) < x < a, where \n",
    "                    a-1 is the next attribute below the listed (e.g., for a = -1.3 by the USDM classification,\n",
    "                    then a-1 is the next attribute down the list (-1.6), so the label estimate is for -1.6 < x < -1.3)\n",
    "           Column 4 is the current depth of the tree for that entry\n",
    "           Column 5 is the current leaf for ths current branch\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Initialize current node of the tree\n",
    "    if tree_depth == 0:\n",
    "        tree = np.zeros((1, 5))\n",
    "        tree = np.append(tree, np.asarray([[None, None, None, tree_depth, leaf],]), axis = 0)\n",
    "        tree = np.delete(tree, 0, axis = 0) # Delete the row of zeros\n",
    "        n = tree.shape[0] - 1\n",
    "    else:\n",
    "        #tree[tree_depth+leaf] = [None, A_best, A[a], tree_depth, a]\n",
    "        tree[n] = [None, A_best, A[a], tree_depth, a]\n",
    "    \n",
    "    # Get the total size of the data\n",
    "    S_norm = y.size - np.sum(np.isnan(y))\n",
    "\n",
    "    # Determine if the root node is pure\n",
    "    if len(y[y == 1]) == S_norm: # All examples are positive\n",
    "        #tree[tree_depth+leaf] = [1, A_best, A[a], tree_depth, a]\n",
    "        tree[n] = [1, A_best, A[a], tree_depth, a]\n",
    "        return tree\n",
    "    elif len(y[y == -1]) == S_norm: # All examples are negative\n",
    "        #tree[tree_depth+leaf] = [-1, A_best, A[a], tree_depth, a]\n",
    "        tree[n] = [-1, A_best, A[a], tree_depth, a]\n",
    "        return tree\n",
    "\n",
    "    # If the attribute list is empty, or the tree is at its max depth, return the stump with the most common value\n",
    "    if (np.sum(~np.isnan(attributes)) < 1) | (tree_depth >= max_depth): \n",
    "        most_common = 1 if (multiplier*np.sum(y == 1)) > np.sum(y == -1) else -1\n",
    "        #tree[tree_depth+leaf] = [most_common, A_best, A[a], tree_depth, a]\n",
    "        tree[n] = [most_common, A_best, A[a], tree_depth, a] \n",
    "        return tree\n",
    "    \n",
    "    V, M = attributes.shape\n",
    "    \n",
    "    # Following if block makes it so that attributes can be reused. Seemed needlessly complicated for 2 attributes, so it was left out for now.\n",
    "    #if tree_depth == 0:\n",
    "    #    A_best = BestAttribute(x, y, attributes)\n",
    "    #    A = attributes[:,A_best]\n",
    "    #else: # For none root branches, do not repeat the attribute just used.\n",
    "    #    A_new = np.zeros((V, M))\n",
    "    #    A_new[:,:] = attributes\n",
    "    #    A_new[:,A_best] = np.nan\n",
    "    #    \n",
    "    #    A_best = BestAttribute(x, y, A_new)\n",
    "    #    A = attributes[:,A_best]\n",
    "    \n",
    "    A_best = BestAttribute(x, y, attributes)\n",
    "    A = attributes[:,A_best]\n",
    "    \n",
    "    A_new = np.zeros((V, M))\n",
    "    A_new[:,:] = attributes\n",
    "    A_new[:,A_best] = np.nan\n",
    "    \n",
    "    # Loop over all values for the best attribute\n",
    "    for a in range(len(A)):\n",
    "        # Split the data\n",
    "        if (a == 0):\n",
    "            xa = x[x[:,A_best] < A[a]]\n",
    "            ya = y[x[:,A_best] < A[a]]\n",
    "            leaf = 0 # If starting at a new set of attributes, reset the leaf\n",
    "        else:\n",
    "            xa = x[(x[:,A_best] >= A[a-1]) & (x[:,A_best] < A[a])]\n",
    "            ya = y[(x[:,A_best] >= A[a-1]) & (x[:,A_best] < A[a])]\n",
    "            leaf = a # Set the current leaf to whatever attribute index is being worked on\n",
    "            \n",
    "        # Add a node below the tree\n",
    "        tree = np.append(tree, np.asarray([[None, A_best, A[a], tree_depth+1, a],]), axis = 0)\n",
    "        n = tree.shape[0] - 1\n",
    "        \n",
    "        # If the examples are empty, assign the most common value\n",
    "        if ya.size == 0:\n",
    "            Sv_norm = 0\n",
    "        else: # Note, theoretically, the Sv_norm < 1 condition is unneeded. But calculating it here helps accout for some sea grid points that may be nan\n",
    "            Sv_norm = ya.size - np.sum(np.isnan(ya))\n",
    "\n",
    "        if Sv_norm < 1:\n",
    "            most_common = 1 if (multiplier*np.sum(ya == 1)) > np.sum(ya == -1) else -1\n",
    "            #tree[tree_depth+leaf+1] = [most_common, A_best, A[a], tree_depth+1, a]\n",
    "            tree[n] = [most_common, A_best, A[a], tree_depth+1, a]\n",
    "            \n",
    "        # If the subset is not empty, grow the tree further with that subset.\n",
    "        else:\n",
    "            tree = DecisionTreeTrain(xa, ya, A_new, max_depth = max_depth, multiplier = multiplier, tree_depth = tree_depth + 1, \n",
    "                                     tree = tree, leaf = a, A = A, a = a, A_best = A_best, n = n)\n",
    "            #tree = DecisionTreeTrain(xa, ya, attributes, max_depth = max_depth, tree_depth = tree_depth + 1, \n",
    "            #                         tree = tree, leaf = a, A = A, a = a, A_best = A_best, n = n)\n",
    "    \n",
    "    return tree\n",
    "        \n",
    "\n",
    "\n",
    "# Function to determine the best attribute to split the data\n",
    "def BestAttribute(x, y, attributes):\n",
    "    '''\n",
    "    Input:\n",
    "    x - Training examples. x is a m x n matrix, with n examples and m  features\n",
    "    y - Labels, or target values. Currently only binary 0 or 1 (or -1 and 1) allowed. Must be a vector of size m\n",
    "    attributes: List of numerical values to compare the training examples to. Currently, only a v x m array of numerical values are accepted.\n",
    "    max_depth: Default = 5. Maxmium depth of the tree\n",
    "    \n",
    "    Output:\n",
    "    A - Best attribute to split the data\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    M, N = x.shape\n",
    "    V, N = attributes.shape\n",
    "    \n",
    "    # Get the total size\n",
    "    if y.size == 0:\n",
    "        S_norm = 0\n",
    "    else:\n",
    "        S_norm = y.size - np.sum(np.isnan(y))\n",
    "    \n",
    "    # Get the probability of positive and negative labels\n",
    "    P_pos = np.sum(y == 1)/S_norm\n",
    "    P_neg = np.sum(y == -1)/S_norm\n",
    "\n",
    "    # Initialize the gain to the entropy\n",
    "    gain = np.zeros((N))\n",
    "    \n",
    "    gain[:] = -1 * P_pos * np.log2(P_pos + 1e-5) - P_neg * np.log2(P_neg + 1e-5)\n",
    "    \n",
    "    for n in range(N):\n",
    "        if np.sum(~np.isnan(attributes[:,n])) < 1:\n",
    "            gain[n] = np.nan\n",
    "            continue\n",
    "            \n",
    "        for v in range(V):\n",
    "            # Split the data\n",
    "            if (v == 0):\n",
    "                Sv = y[x[:,n] < attributes[v,n]]\n",
    "            else:\n",
    "                Sv = y[(x[:,n] >= attributes[v-1,n]) & (x[:,n] < attributes[v,n])]\n",
    "            \n",
    "            # Get the size of each split\n",
    "            if Sv.size == 0: # If the split dataset is empty, it does not contribute to the gain (SV_norm = 0)\n",
    "                gain[n] = gain[n] \n",
    "            else:\n",
    "                Sv_norm = Sv.size - np.sum(np.isnan(Sv))\n",
    "\n",
    "                # Calculate probabilities\n",
    "                Pv_pos = np.sum(Sv == 1)/Sv_norm\n",
    "                Pv_neg = np.sum(Sv == -1)/Sv_norm\n",
    "\n",
    "                # Calculate the entropy after each split\n",
    "                Ev = -1 * Pv_pos * np.log2(Pv_pos + 1e-5) - Pv_neg * np.log2(Pv_neg + 1e-5)\n",
    "\n",
    "                # Calculate the gain for this attribute\n",
    "                gain[n] = gain[n] - Sv_norm * Ev/S_norm\n",
    "        \n",
    "    \n",
    "    # Calculate the best attribute\n",
    "    A = np.where(gain == np.nanmax(gain))[0][0]\n",
    "    #A = attributes[gain == np.max(gain)]\n",
    "    \n",
    "    return A\n",
    "\n",
    "    \n",
    "# Function to make predictions with a decision tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
